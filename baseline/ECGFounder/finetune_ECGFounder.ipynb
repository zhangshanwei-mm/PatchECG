{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import os\n",
    "from shutil import copyfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import json\n",
    "from util import save_checkpoint, save_reg_checkpoint, my_eval_with_dynamic_thresh\n",
    "from finetune_model import ft_12lead_ECGFounder, ft_1lead_ECGFounder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dataset import LVEF_12lead_cls_Dataset, LVEF_12lead_reg_Dataset, LVEF_1lead_cls_Dataset, LVEF_1lead_reg_Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune ECG for LVEF classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lead = 12 # 12-lead ECG or 1-lead ECG \n",
    "\n",
    "gpu_id = 4\n",
    "batch_size = 512\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "early_stop_lr = 1e-5\n",
    "Epochs = 5\n",
    "df_label_path = './csv/LVEF.csv'\n",
    "ecg_path = 'your_path/mimic-iv-ecg-diagnostic-electrocardiogram-matched-subset-1.0/'\n",
    "tasks = ['class']\n",
    "saved_dir = './res/eval/'\n",
    "\n",
    "device = torch.device('cuda:{}'.format(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_classes = len(tasks)\n",
    "\n",
    "if num_lead == 12:\n",
    "  ECGdataset = LVEF_12lead_cls_Dataset()\n",
    "  pth = './checkpoint/12_lead_ECGFounder.pth'\n",
    "  model = ft_12lead_ECGFounder(device, pth, n_classes,linear_prob=False)\n",
    "elif num_lead == 1:\n",
    "  ECGdataset = LVEF_1lead_cls_Dataset()\n",
    "  pth = './checkpoint/1_lead_ECGFounder.pth'\n",
    "  model = ft_1lead_ECGFounder(device, pth, n_classes,linear_prob=False)\n",
    "\n",
    "df_label = pd.read_csv(df_label_path)\n",
    "# Splitting the dataset into train, validation, and test sets\n",
    "\n",
    "train_df, test_df = train_test_split(df_label, test_size=0.2, shuffle=False)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_dataset = ECGdataset(ecg_path= ecg_path,labels_df=train_df)\n",
    "val_dataset = ECGdataset(ecg_path= ecg_path,labels_df=val_df)\n",
    "test_dataset = ECGdataset(ecg_path= ecg_path,labels_df=test_df)\n",
    "\n",
    "# Example DataLoader usage\n",
    "trainloader = DataLoader(train_dataset, batch_size=256,num_workers=40, shuffle=True)\n",
    "valloader = DataLoader(test_dataset, batch_size=256,num_workers=40, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=256,num_workers=40, shuffle=False)\n",
    "\n",
    "# linear classificaion  ->  linear_prob=True\n",
    "# full fine-tuning  ->  linear_prob=False\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1, mode='max', verbose=True)\n",
    "\n",
    "### train model\n",
    "best_val_auroc = 0.\n",
    "step = 0\n",
    "current_lr = lr\n",
    "all_res = []\n",
    "pos_neg_counts = {}\n",
    "total_steps_per_epoch = len(trainloader)\n",
    "eval_steps = total_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(Epochs):\n",
    "    ### train\n",
    "    for batch in tqdm(trainloader,desc='Training'):\n",
    "        input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(input_x)\n",
    "        loss = criterion(outputs, input_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % eval_steps == 0:\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            prog_iter_val = tqdm(testloader, desc=\"Validation\", leave=False)\n",
    "            all_gt = []\n",
    "            all_pred_prob = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(prog_iter_val):\n",
    "                    input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "                    logits = model(input_x)\n",
    "                    pred = torch.sigmoid(logits)\n",
    "                    all_pred_prob.append(pred.cpu().data.numpy())\n",
    "                    all_gt.append(input_y.cpu().data.numpy())\n",
    "            all_pred_prob = np.concatenate(all_pred_prob)\n",
    "            all_gt = np.concatenate(all_gt)\n",
    "            all_gt = np.array(all_gt)\n",
    "            res_val, res_val_auroc, res_test_sens, res_test_spec, res_test_f1, res_test_auprc, thre = my_eval_with_dynamic_thresh(all_gt, all_pred_prob)\n",
    "            val_auroc = res_val\n",
    "            print('Epoch {} step {}, val: {:.4f}'.format(epoch, step, res_val))\n",
    "\n",
    "            # test\n",
    "            model.eval()\n",
    "            prog_iter_test = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "            all_gt = []\n",
    "            all_pred_prob = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(prog_iter_test):\n",
    "                    input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "                    logits = model(input_x)\n",
    "                    pred = torch.sigmoid(logits)\n",
    "                    all_pred_prob.append(pred.cpu().data.numpy())\n",
    "                    all_gt.append(input_y.cpu().data.numpy())\n",
    "            all_pred_prob = np.concatenate(all_pred_prob)\n",
    "            all_gt = np.concatenate(all_gt)\n",
    "            all_gt = np.array(all_gt)\n",
    "            res_test, res_test_auroc, res_test_sens, res_test_spec, res_test_f1, res_test_auprc, thre = my_eval_with_dynamic_thresh(all_gt, all_pred_prob)\n",
    "            \n",
    "            print('Epoch {} step {}, val: {:.4f}, test: {:.4f} '.format(epoch, step, res_val, res_test))\n",
    "\n",
    "            ### save model and res\n",
    "            is_best = bool(val_auroc > best_val_auroc)\n",
    "            if is_best:\n",
    "                best_val_auroc = val_auroc\n",
    "                print('==> Saving a new val best!')\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'val_auroc': val_auroc,\n",
    "                }, saved_dir)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            #all_res.append(list(res_test_auroc))\n",
    "\n",
    "            for i, task in enumerate(tasks):\n",
    "              pos_count = test_df[task].sum()\n",
    "              neg_count = len(test_df) - pos_count\n",
    "              all_res.append([task, res_test_auroc[i], res_test_sens[i], res_test_spec[i], res_test_f1[i], res_test_auprc[i], thre[i], pos_count, neg_count])\n",
    "\n",
    "            columns = ['Field_ID', 'AUROC', 'sensitivity', 'specificity', 'f1', 'auprc', 'thre', 'pos_num','neg_num']\n",
    "            \n",
    "            \n",
    "            df = pd.DataFrame(all_res, columns=columns)\n",
    "\n",
    "            df.to_csv(os.path.join(saved_dir, f'res.csv'), index=False, float_format='%.5f')\n",
    "            \n",
    "            scheduler.step(val_auroc)\n",
    "            ### early stop\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            if current_lr < early_stop_lr:\n",
    "                print(\"Early stop\")\n",
    "                exit()\n",
    "                \n",
    "            model.train() # set back to train\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune ECG for LVEF regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lead = 12 # 12-lead ECG or 1-lead ECG \n",
    "\n",
    "gpu_id = 4\n",
    "batch_size = 512\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "early_stop_lr = 1e-5\n",
    "Epochs = 5\n",
    "df_label_path = './csv/LVEF.csv'\n",
    "ecg_path = '/hot_data/lijun/data/mimic-iv-ecg-diagnostic-electrocardiogram-matched-subset-1.0/'\n",
    "tasks = ['class']\n",
    "saved_dir = './res/eval/'\n",
    "\n",
    "device = torch.device('cuda:{}'.format(gpu_id) if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "n_classes = len(tasks)\n",
    "\n",
    "if num_lead == 12:\n",
    "  ECGdataset = LVEF_12lead_reg_Dataset()\n",
    "  pth = './checkpoint/12_lead_ECGFounder.pth'\n",
    "  model = ft_12lead_ECGFounder(device, pth, n_classes,linear_prob=False)\n",
    "elif num_lead == 1:\n",
    "  ECGdataset = LVEF_1lead_reg_Dataset()\n",
    "  pth = './checkpoint/1_lead_ECGFounder.pth'\n",
    "  model = ft_1lead_ECGFounder(device, pth, n_classes,linear_prob=False)\n",
    "\n",
    "df_label = pd.read_csv(df_label_path)\n",
    "# Splitting the dataset into train, validation, and test sets\n",
    "\n",
    "train_df, test_df = train_test_split(df_label, test_size=0.2, shuffle=False)\n",
    "val_df, test_df = train_test_split(test_df, test_size=0.5, shuffle=False)\n",
    "\n",
    "train_dataset = ECGdataset(ecg_path= ecg_path,labels_df=train_df)\n",
    "val_dataset = ECGdataset(ecg_path= ecg_path,labels_df=val_df)\n",
    "test_dataset = ECGdataset(ecg_path= ecg_path,labels_df=test_df)\n",
    "\n",
    "# Example DataLoader usage\n",
    "trainloader = DataLoader(train_dataset, batch_size=256,num_workers=40, shuffle=True)\n",
    "valloader = DataLoader(test_dataset, batch_size=256,num_workers=40, shuffle=False)\n",
    "testloader = DataLoader(test_dataset, batch_size=256,num_workers=40, shuffle=False)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.1, mode='max', verbose=True)\n",
    "\n",
    "### train model\n",
    "best_mae = 100.\n",
    "step = 0\n",
    "current_lr = lr\n",
    "all_res = []\n",
    "pos_neg_counts = {}\n",
    "total_steps_per_epoch = len(trainloader)\n",
    "eval_steps = total_steps_per_epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(Epochs):\n",
    "    ### train\n",
    "    for batch in tqdm(trainloader,desc='Training'):\n",
    "        input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "        outputs = model(input_x)\n",
    "        loss = criterion(outputs, input_y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        step += 1\n",
    "\n",
    "        if step % eval_steps == 0:\n",
    "\n",
    "            # val\n",
    "            model.eval()\n",
    "            prog_iter_val = tqdm(valloader, desc=\"Validation\", leave=False)\n",
    "            all_gt = []\n",
    "            all_pred_prob = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(prog_iter_val):\n",
    "                    input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "                    pred = model(input_x)\n",
    "                    all_pred_prob.append(pred.cpu().data.numpy())\n",
    "                    all_gt.append(input_y.cpu().data.numpy())\n",
    "            all_pred_prob = np.concatenate(all_pred_prob)\n",
    "            all_gt = np.concatenate(all_gt)\n",
    "            all_gt = np.array(all_gt)\n",
    "            val_mae = np.mean(np.abs(all_pred_prob - all_gt))\n",
    "            rmse = np.sqrt(np.mean((all_pred_prob - all_gt) ** 2))\n",
    "\n",
    "            print(f'MAE: {val_mae}')\n",
    "            print(f'RMSE: {rmse}')\n",
    "\n",
    "            # test\n",
    "            model.eval()\n",
    "            prog_iter_test = tqdm(testloader, desc=\"Testing\", leave=False)\n",
    "            all_gt = []\n",
    "            all_pred_prob = []\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, batch in enumerate(prog_iter_test):\n",
    "                    input_x, input_y = tuple(t.to(device) for t in batch)\n",
    "                    pred = model(input_x)\n",
    "                    #pred = torch.sigmoid(logits)\n",
    "                    all_pred_prob.append(pred.cpu().data.numpy())\n",
    "                    all_gt.append(input_y.cpu().data.numpy())\n",
    "            all_pred_prob = np.concatenate(all_pred_prob)\n",
    "            all_gt = np.concatenate(all_gt)\n",
    "            all_gt = np.array(all_gt)\n",
    "            mae = np.mean(np.abs(all_pred_prob - all_gt))\n",
    "            rmse = np.sqrt(np.mean((all_pred_prob - all_gt) ** 2))\n",
    "\n",
    "            ### save model and res\n",
    "            is_best = bool(val_mae < best_mae)\n",
    "            if is_best:\n",
    "                best_mae = val_mae\n",
    "                print('==> Saving a new val best!')\n",
    "                save_reg_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'step': step,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'scheduler': scheduler.state_dict(),\n",
    "                    'mae': val_mae,\n",
    "                }, saved_dir)\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "            columns = ['mae', 'rmse']\n",
    "            \n",
    "            all_res.append([mae, rmse])\n",
    "            df = pd.DataFrame(all_res, columns=columns)\n",
    "\n",
    "            df.to_csv(os.path.join(saved_dir, f'res_reg.csv'), index=False, float_format='%.5f')\n",
    "            \n",
    "            scheduler.step(rmse)\n",
    "            ### early stop\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "                \n",
    "            model.train() # set back to train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ecg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
